---
title: "Customer Segmentation"
output: github_document
---

What is Customer Segmentation?
Customer segmentation is the practice of dividing a customer base into groups of individuals that are similar in specific ways relevant to marketing, such as age, gender, interests and spending habits.

Benefits of customer segmentation
By enabling companies to target specific groups of customers, a customer segmentation model allows for the effective allocation of marketing resources and the maximization of cross- and up-selling opportunities.

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(tidyverse) 
library(cluster)  
library(factoextra)
library(GGally)
library(funModeling)
```



## Including Code

You can include R code in the document as follows:

```{r}

customer_data=read.csv("C:/Users/senih/OneDrive/Desktop/customer_segmentation_dataset/Mall_Customers.csv")
str(customer_data)



```

## Understand the data

We will now display the first six rows of our dataset using the head() function

```{r}
head(customer_data)

```



##Correlation matrixes  (understand the correlation between variables)
```{r}

ggpairs(customer_data[ , 3:5])

```

According to the graph above the correlation between variables are not high.  

## Understand the data
```{r}
profiling_num(customer_data)
```
According to the table above there is no need to standardize the data. Because the variances of the variables are pretty close to each other.


```{r}
summary(customer_data$Age)
```

```{r}
sd(customer_data$Spending.Score..1.100.)

```

## Customer Gender Visualization
```{r}
gender_visual = table(customer_data$Gender)

barplot(gender_visual,main="Gender Comparision",
       ylab="Count",
       xlab="Gender",
       col=rainbow(2),
       legend=rownames(gender_visual))
```

We observed that the number of females is higher than the males. Now, let us visualize a pie chart to observe the ratio of male and female distribution.

```{r}
pct=round(gender_visual/sum(gender_visual)*100)
lbs=paste(c("Female","Male")," ",pct,"%",sep=" ")
library(plotrix)
pie3D(gender_visual,labels=lbs,
   main="Ratio of Female and Male")
```

From the above graph, we conclude that the percentage of females is 56%, whereas the percentage of male in the customer dataset is 44%.


##  Visualization of Age Distribution

We will first proceed by taking summary of the Age variable.
```{r}
summary(customer_data$Age)
```


```{r}
hist(customer_data$Age,
    col="blue",
    main="Count of Age Class",
    xlab="Age Class",
    ylab="Frequency",
    labels=TRUE)
```

```{r}
boxplot(customer_data$Age,
       col="Blue",
       main="Descriptive Analysis of Age")
```

From the above two visualizations, we conclude that the maximum customer ages are between 30 and 35. The minimum age of customers is 18, whereas, the maximum age is 70.


## Analysis of the Annual Income of the Customers
In this section of the R project, we will create visualizations to analyze the annual income of the customers. We will plot a histogram and then we will proceed to examine this data using a density plot.

```{r}
summary(customer_data$Annual.Income..k..)
```


```{r}
hist(customer_data$Annual.Income..k..,
  col="Blue",
  main="Annual Income",
  xlab="Annual Income Class",
  ylab="Frequency",
  labels=TRUE)
```


```{r}
plot(density(customer_data$Annual.Income..k..),
    col="yellow",
    main="Density Plot for Annual Income",
    xlab="Annual Income Class",
    ylab="Density")
polygon(density(customer_data$Annual.Income..k..),
        col="#ccff66")
```

From the above descriptive analysis, we conclude that the minimum annual income of the customers is 15 and the maximum income is 137. People earning an average income of 70 have the highest frequency count in our histogram distribution. The average salary of all the customers is 60.56. In the Kernel Density Plot that we displayed above, we observe that the annual income has a normal distribution.


## Analyzing Spending Score of the Customers

```{r}
summary(customer_data$Spending.Score..1.100.)
```


```{r}
boxplot(customer_data$Spending.Score..1.100.,
   horizontal=TRUE,
   col="Red",
   main="Descriptive Analysis of Spending Score")
```
```{r}
hist(customer_data$Spending.Score..1.100.,
    main="Spending Score",

        xlab="Spending Score Class",
    ylab="Frequency",
    col="Red",
    labels=TRUE)
```

The minimum spending score is 1, maximum is 99 and the average is 50.20. We can see Descriptive Analysis of Spending Score is that Min is 1, Max is 99 and avg. is 50.20. From the histogram, we conclude that customers between class 40 and 50 have the highest spending score among all the classes.


##  K-means Algorithm
While using the k-means clustering algorithm, the first step is to indicate the number of clusters (k) that we wish to produce in the final output. 

###  Finding optimum cluster number with Elbow method


```{r}
set.seed(123)

fviz_nbclust(customer_data[ , 3:5], kmeans, method = "wss")

```

From the above graph, we conclude that 6 is the appropriate number of clusters since it seems to be appearing at the bend in the elbow plot.


###  Finding optimum cluster number with Silhouette method
```{r}
fviz_nbclust(customer_data[ , 3:5], kmeans, method = "silhouette")
```

As you see from the above graph, we conclude that 6 is the appropriate number of clusters according to silhouette method.


###  Finding optimum cluster number with Gap statistics method
```{r}
set.seed(123)
gap_stat <- clusGap(customer_data[ , 3:5], FUN = kmeans, nstart = 25, K.max = 10, B = 50)
print(gap_stat, method = "firstmax")

fviz_gap_stat(gap_stat)

```

As you see from the above graph, we conclude that 6 is the appropriate number of clusters according to Gap statistics method.


All three methods shows us the optimum cluster number should be 6. 
Now, let us take k = 6 as our optimal cluster.

## K Means CLuster
```{r}
set.seed(123)
final <- kmeans(customer_data[ , 3:5], 6, nstart = 25)
print(final)

```

In the output of our kmeans operation, we observe a list with several key information. From this, we conclude the useful information being 

-cluster – This is a vector of several integers that denote the cluster which has an allocation of each point.
-totss – This represents the total sum of squares.
-centers – Matrix comprising of several cluster centers
-withinss – This is a vector representing the intra-cluster sum of squares having one component per cluster.
-tot.withinss – This denotes the total intra-cluster sum of squares.
-betweenss – This is the sum of between-cluster squares.
-size – The total number of points that each cluster holds.


## Combine cluster data with the customer_data
```{r}
cbind(customer_data, Cluster = final$cluster) -> final_data  
print(final_data) # see the table
```


## VISUALIZE CLUSTERS
```{r}
fviz_cluster(final, data = customer_data[ , 3:5])

```



## Descriptive statistics according to clusters
```{r}
customer_data[ , 3:5] %>%
  mutate(Cluster = final$cluster) %>%
  group_by(Cluster) %>%
  summarise_all("mean")
```

As you see from the above table, we see mean values of each cluster.


##  Visualizing the Clustering Results using the First Two Principle Components
```{r}

pcclust=prcomp(customer_data[,3:5],scale=FALSE) # 3 PC are created (principal component analysis)
summary(pcclust)

```

The higher variance means the higher importance. So we will use PC1 and PC2 below.

```{r}
pcclust$rotation[,1:2] 
```

If the PC number is positive, the variable positively contributes to the component. If it’s negative, then they are negatively related. Larger the number, stronger the relationship. Spending score variable in PC1 has negatively strong relationship. And Annual Income in PC2 has negatively strong relationship.


## Visualizing clusters according to Annual Income and Spending Score
```{r}
set.seed(1)
ggplot(customer_data, aes(x =Annual.Income..k.., y = Spending.Score..1.100.)) + 
  geom_point(stat = "identity", aes(color = as.factor(final$cluster))) +
  scale_color_discrete(name=" ",
              breaks=c("1", "2", "3", "4", "5","6"),
              labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```

From the above visualization, we observe that there is a distribution of 6 clusters as follows –

-Cluster 6 and 5 – These clusters represent the customer_data with the medium income salary as well as the medium annual spend of salary.

-Cluster 3 – This cluster represents the customer_data having a high annual income as well as a high annual spend.

-Cluster 1 – This cluster denotes the customer_data with low annual income as well as low yearly spend of income.

-Cluster 4 – This cluster denotes a high annual income and low yearly spend.

-Cluster 2 – This cluster represents a low annual income but its high yearly expenditure.


## Visualizing clusters according to Age and Spending Score
```{r}
ggplot(customer_data, aes(x =Spending.Score..1.100., y =Age)) + 
  geom_point(stat = "identity", aes(color = as.factor(final$cluster))) +
  scale_color_discrete(name=" ",
                      breaks=c("1", "2", "3", "4", "5","6"),
                      labels=c("Cluster 1", "Cluster 2", "Cluster 3", "Cluster 4", "Cluster 5","Cluster 6")) +
  ggtitle("Segments of Mall Customers", subtitle = "Using K-means Clustering")
```


```{r}
kCols=function(vec){cols=rainbow (length (unique (vec)))
return (cols[as.numeric(as.factor(vec))])}

digCluster<-final$cluster; 
dignm<-as.character(digCluster); # K-means clusters

plot(pcclust$x[,1:2], 
     col =kCols(digCluster),
     pch =19,xlab ="K-means",
     ylab="classes")
legend("bottomleft",unique(dignm),
       fill=unique(kCols(digCluster)))



```

Cluster 5 and 6 – These two clusters consist of customers with medium PCA1 and medium PCA2 score.

Cluster 3 – This cluster represents customers having a high PCA2 and a low PCA1.

Cluster 4 – In this cluster, there are customers with a medium PCA1 and a low PCA2 score.

Cluster 1 – This cluster comprises of customers with a high PCA1 income and a high PCA2.

Cluster 2 – This comprises of customers with a high PCA2 and a medium annual spend of income.

With the help of clustering, we can understand the variables much better, prompting us to take careful decisions. With the identification of customers, companies can release products and services that target customers based on several parameters like income, age, spending patterns, etc. Furthermore, more complex patterns like product reviews are taken into consideration for better segmentation.


Summary
In this data science project, we went through the customer segmentation model. We developed this using a class of machine learning known as unsupervised learning. Specifically, we made use of a clustering algorithm called K-means clustering. We analyzed and visualized the data and then proceeded to implement our algorithm.

